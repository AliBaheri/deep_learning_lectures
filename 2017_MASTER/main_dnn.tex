\documentclass[aspectratio=169]{beamer}
\mode<presentation>
{
  \usetheme{metropolis}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{default} % or try albatross, beaver, crane, ...
  \usefonttheme{structurebold}  % or try serif, structurebold, ...
  \setbeamercolor{background canvas}{bg=white}
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{bibliography item}{\insertbiblabel}
  %\setbeamertemplate{caption}[numbered]
} 
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{listings}             % Include the listings-package
\hypersetup{
    colorlinks = true,
    linkcolor = {black},
    urlcolor = {blue}
}

\DeclareMathOperator*{\argmin}{arg\,min}

\title[Deep Learning and Temporal Data Processing]{Deep Learning and Temporal Data Processing}
\subtitle{1 - Deep Neural Networks}
\institute{University of Modena and Reggio Emilia}
\author{Andrea Palazzi}
\date{June 21th, 2017}

\def\thisframelogos{}

\newcommand{\framelogo}[1]{\def\thisframelogos{#1}}
\newcommand{\R}{\mathbb{R}}

\addtobeamertemplate{frametitle}{}{%
\begin{tikzpicture}[remember picture,overlay]
\node[anchor=north east] at (current page.north east) {%
    \foreach \img in \thisframelogos {%
        %\hspace{.5ex}%
        \includegraphics[height=3.5ex]{\img}%
    }%
};
\end{tikzpicture}}

\begin{document}

\framelogo{logo_unimore_white.png}

\include{src/titlepage}
\include{src/outline}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Deep Neural Networks: overview}
\cite{yu2015multi}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Linear classifiers}

\begin{frame}{Linear Classifier}
For the purpose of this lecture, we'll stick to the task of image classification.\\
Let's assume we have a training dataset of $N$ images
\begin{equation*}
x_i \in \R^D, i = 1, \dots, N
\end{equation*}
that we want to classify into $K$ distinct classes.\\
Thus, training set is made by couples:
\begin{equation*}
(x_i, y_i),\ where \, y_i \in \{1, \dots, K\}
\end{equation*}
Our goal is to define a function $f: \R^D \mapsto \R^K$ that maps images to class scores.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Linear Classifier}
Making a real-world example: let's take the \texttt{CIFAR-10} dataset, which consists of $N=60000$ 32x32 RGB images belonging to 10 different classes.\\
\begin{figure}
\begin{tabular}{c}
\includegraphics[width=0.4\textwidth]{img/dnn/cifar10.png}
\end{tabular}
\end{figure}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Linear Classifier}
Each image is $32$x$32$x$3$, thus it can be thought as a column vector $x_i \in \R^{3072}$.\\
Now we can define a \textbf{linear mapping}:
\begin{equation*}
f(x_i,W,b) = Wx_i + b
\end{equation*}
where the parameters are:
\begin{itemize}
\item the weight matrix $W \in \R^{10 x 3072}$ 
\item the bias vector $b \in \R^{10}$.
\end{itemize}
\small{Intuitively, our goal is to learn the parameters from the training set \emph{s.t.} when a new test image $x^{test}_i$ is given as input, the score of the correct class is higher that the scores of other classes.}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\

\begin{frame}{Linear Classifier}
\begin{figure}
\begin{tabular}{c}
\includegraphics[width=0.8\textwidth]{img/dnn/linear_classifier.jpg}
\end{tabular}
\end{figure}
\small{Example of mapping an image to a score. For the sake of visualization, here the image is assumed to have only 4 grayscale pixels.}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\

\begin{frame}{Logistic Regression}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\

\begin{frame}{Softmax Classifier}
First let's introduce the \textbf{softmax function}:
\begin{equation*}
softmax_j(\textbf{z}) = \frac{e^{z_j}}{\sum_k e^{z_k}}
\end{equation*}
It takes a vector of arbitrary real-valued scores $\textbf{z}$ and squashes it to a vector of values between zero and one that sum to one.\\
\emph{e.g.}
\begin{equation*}
\textbf{z} = \begin{bmatrix}1.2\\5.1\\2.7\end{bmatrix}
\quad softmax(\textbf{z}) = \begin{bmatrix}0.018\\0.90\\0.08\end{bmatrix}
\end{equation*}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\

\begin{frame}{Softmax Classifier}
\textbf{Softmax Classifier} generalizes Logistic Regression classifier to multi-class classification.\\
\vspace{0.2cm}
In the Softmax classifier the scores of linear function mapping $f(x_i,W) = Wx_i$ are interpreted as unnormalized log probabilities and we use the \textbf{cross-entropy loss}:
\begin{equation*}
L_i = -log\left(\frac{e^{f_{y_{i}}}}{\sum_j e^{f_{j}}}\right)
\end{equation*}
\small{Prove yourself that this loss function makes sense.}
%Softmax Classifier has the appealing property to produce an easy-to-interpret output, that is the normalized score confidence for each class.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Modeling a Neuron}

\begin{frame}{Introduction}
Neural Networks are a mathematical model \textbf{coarsely} inspired by the way our own brain works.
\begin{figure}
\begin{tabular}{c}
\includegraphics[width=0.4\textwidth]{img/dnn/biological_neuron.jpg}
\end{tabular}
\end{figure}
\small{Neurons are the basic computational unit of our brain. Approximately 86 billion neurons can be found in the human nervous system and they are connected with approximately $10^{14}$ - $10^{15}$ \textbf{synapses}. Each neuron receives input signals from its \textbf{dendrites} and produces output signals along its (single) \textbf{axon}. The axon eventually branches out and connects via synapses to dendrites of other neurons.}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Modeling a Single Neuron}
More formally, we can model a single \textbf{neuron} as follows:
\begin{figure}
\begin{tabular}{c}
\includegraphics[width=0.4\textwidth]{img/dnn/single_neuron.jpg}
\end{tabular}
\end{figure}
Each neuron can have multiple inputs. The neuron's output is the dot product between the inputs and its weights, plus the bias: then, a non-linearity is applied.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Modeling a Single Neuron}
It's easy to see that a single neuron can be used to implement a binary classifier.\\
\vspace{1cm}
Indeed, when \textbf{cross-entropy loss} is applied to neuron's output, we can optimize a \textbf{binary Softmax classifier} (\emph{a.k.a.} Logistic regression).
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Neuron Activation}

\textbf{Activation functions} are non-linear functions computed on the output of each neuron.\\
There are a number of different activation functions you could use. In practice, the three most widely used functions have been \textit{sigmoid}, \textit{tanh} and \textit{ReLu}. Nonetheless, more complex activation functions exist (\emph{e.g.} \cite{he2015delving,goodfellow2013maxout}).

\textbf{Why do we bother using activations?}\\
\pause
Composition of linear functions is a linear function. Without nonlinearities, neural networks would reduce to 1 layer logistic regression.

%\begin{figure}
%\begin{tabular}{ccc}
%\includegraphics[width=0.3\textwidth]{img/cnn/act_sigmoid.jpg}&
%\includegraphics[width=0.3\textwidth]{img/cnn/act_tanh.jpg} & 
%\includegraphics[width=0.3\textwidth]{img/cnn/act_relu.jpg}\\
%Sigmoid & Tanh & ReLu
%\end{tabular}
%\end{figure}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Neuron Activation}
\begin{columns}
\begin{column}{0.25\textwidth}
\begin{tabular}{c}
\\
\includegraphics[width=\textwidth]{img/cnn/act_sigmoid.jpg}\\
Sigmoid activation\\ \\
\includegraphics[width=\textwidth]{img/cnn/act_tanh.jpg}\\
Tanh activation
\end{tabular}
\end{column}
\begin{column}{0.7\textwidth}
\textbf{Sigmoid} nonlinearity has form $\sigma(x) = 1 / (1 + e^{-x})$. Sigmoid function squashes any real-valued input into range $[0, 1]$. It has seen frequent use historically, yet now it's rarely used. It has two major drawbacks:
\begin{itemize}
\item saturates and kill the gradient
\item output is not zero centered
\end{itemize}
\textbf{Tanh} is a scaled sigmoid neuron: $tanh(x)=2\sigma(x)-1$. Differently to sigmoid, input is squashed in range [-1, 1], so the output is 0-centered. However, activation can still saturate and kill the gradient.
\end{column}
\end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Neuron Activation}
\begin{columns}
\begin{column}{0.25\textwidth}
\begin{tabular}{c}
\includegraphics[width=\textwidth]{img/cnn/act_relu.jpg}\\
ReLu activation\\ 
\end{tabular}
\end{column}
\begin{column}{0.75\textwidth}
\textbf{ReLu} stands for Rectified Linear Unit and computes the function $f(x) = max(0, x)$. ReLu activation was found to greatly accelerate the convergence of SGD compared to sigmoid/tanh functions \cite{krizhevsky2012imagenet}. Furthermore, ReLu can be implemented by a simple threshold, \emph{w.r.t.} other activations which require complex operations.
\end{column}
\end{columns}
\end{frame}

%\textbf{Why using non-linear activations at all?}\\
%Composition of linear functions is a linear function. Without nonlinearities, neural networks would reduce to 1 layer logistic regression.
%\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Neural Networks}

\begin{frame}{Neural Networks}
When we connect an ensemble of neurons in an acyclic graph is when the magic happens and we get an actual \textbf{neural network}.
\vspace{0.5cm}
\begin{columns}
\begin{column}{0.48\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{img/dnn/neural_network.jpg}    \end{column}
\begin{column}{0.48\textwidth}
Neural networks are arranged in \textbf{layers}, with one \textit{input layer}, one \textit{output layer} and $N$ \textit{hidden layers} in the middle.\\
\vspace{0.5cm}
\textit{\small{The network depicted here has a total of $47$ learnable parameters. Does this make sense to you?}}
\end{column}
\end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Neural Networks}
The 4-layer network previously depicted can be simply expressed as:
\begin{equation}\label{eq:feedforward}
out = \phi(\mathbf{W_3}\phi(\mathbf{W_2}\phi(\mathbf{W_1x})))
\end{equation}
where:
\begin{columns}
\begin{column}{0.6\textwidth}
\begin{itemize}
\item $\phi$ is the activation function
\item $\mathbf{x} \in \R^{6}$ is the input 
\item $\mathbf{W_1} \in \R^{4x6}$ are the weights of first layer
\item $\mathbf{W_2} \in \R^{3x4}$ are the weights of second layer
\item $\mathbf{W_3} \in \R^{1x3}$ are the weights of third layer
\end{itemize}
\end{column}
\begin{column}{0.4\textwidth}
\includegraphics[width=0.8\textwidth]{img/dnn/neural_network.jpg}   
\end{column}
\end{columns}
\vspace{0.1cm}
\small{Notice that to ease the notation biases have been incorporated into weight matrices W.}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Forward propagation}

\textbf{Forward propagation} is the process of computing the network output given its input.
\begin{figure}
\begin{tabular}{c}
\includegraphics[width=0.5\textwidth]{img/dnn/forward_prop.jpg}
\end{tabular}
\end{figure}
The formula of forward propagation for our toy network above is the one in Eq. \ref{eq:feedforward}.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Representational Power}
It has been shown (\emph{e.g.} \cite{cybenko1989approximation}) that any continuous function $f(x)$ can be approximated at arbitrary precision $\epsilon > 0$ by a neural network $g(x)$ with at least one hidden layer.\\
That is:
\begin{equation*}
\forall x, \quad |f(x) - g(x)| < \epsilon
\end{equation*}
For this reason neural networks with at least one hidden layers are referred to as \textbf{universal approximators}.
In practice however networks with more layers often outperform 2-layer nets, despite the fact that on paper their representational power is equal.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Setting Hyperparameters}
The number of layers in the network, as well as the number of neurons in each layers, are so called \textbf{hyperparameters} of the architecture.\\
\vspace{0.25cm}
Keep in mind that:
\begin{itemize}
\item the bigger the size and the number of layers, the more the \textbf{capacity} of the network increases, which is good because the space of representable functions grow.\\
\item bigger networks without proper regularization are way more prone to \textbf{overfit} the training data, which is bad.
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Setting Hyperparameters}
\begin{columns}
\begin{column}{0.6\textwidth}
\includegraphics[width=0.9\textwidth]{img/dnn/dnn_overfit.jpg}\\
\includegraphics[width=0.9\textwidth]{img/dnn/dnn_overfit_reg.jpg}
\end{column}
\begin{column}{0.4\textwidth}
In practice, usually networks are made as big as computational budget allows.\\
\vspace{0.5cm}
Then overfitting is prevented through proper \textbf{regularization techniques} (\emph{e.g.} dropout, L2 regularization, input noise).
\end{column}
\end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Training a DNN}

\begin{frame}{Backpropagation}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Credits}
\include{src/credits}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{References}

\begin{frame}[t]
\frametitle{References}
\bibliographystyle{abbrv}
\bibliography{bibliography}
\end{frame}
\end{document}