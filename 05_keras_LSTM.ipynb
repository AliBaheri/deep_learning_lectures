{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "In this tutorial we'll learn the basics about sequence modeling using deep recurrent models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time matters\n",
    "In the past tutorials, we learnt how to model time-agnostic data with deep neural networks. For many problems this is not enough. As an example, assume you want to predict the next word in a sentence given the sequence of words up to it. It is very likely that such prediction depends, in some way, on the order such previous words are presented.\n",
    "Recurrent Networks (RNNs) are designed to perform some task for every element of a sequence, and to keep a **state** (memory) that stores information about previous computations. \n",
    "A very common way to picture RNNs is the following:\n",
    "\n",
    "<img src=\"img/rnn.jpg\" width=\"600\">\n",
    "\n",
    "The image shows how a RNN can be unrolled into a full network, meaning that we can write it for the complete sequence. For example, if the sequence we fit is 5 timestamps long, the RNN can be unrolled into a regular feed forward neural network with 5 layers, one for each sequence element. The usual backpropagation algorithm can be applied in this context.\n",
    "\n",
    "**We just need some math now**.\n",
    "\n",
    "* With $x_t$ we denote the input at time step $t$. For example, it could be a one hot vector identifying a certain word in a dictionary;\n",
    "* $s_t$ denotes the hidden state at time step $t$, and encodes the \"memory\". $s_t$ is dependent on the previous hidden state and on the current input: $s_t=\\phi(Ux_t + Ws_{t-1})$. The function $f$ is a non-linearity such as ReLU or tanh. \n",
    "* $o_t$ denotes the output at a step $t$. It depends only on the current state: $o_t = \\phi(Vs_t)$\n",
    "\n",
    "### Drawbacks\n",
    "There are mainly two issues when training a RNN. \n",
    "The first one is the following. As the length of the sequence grows, the \"unrolled\" version of the net becomes very deep, and suffers **exploding or vanishing gradients**.\n",
    "Moreover, RNNs are capable of modeling the state in arbitrarily long sequences just in theory. When it comes to reality, they just look a **few steps** back in the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long short term memory\n",
    "To overcome the issues affecting recurrent networks, usually a gated unit is used. One famous example is the Long Short Term Memory (LSTM) model. This model defines a special kind of RNN, capable of learning long-term dependencies. It works tremendously well on a large variety of problems, and are now widely used.\n",
    "Their behavior is defined by a cell storing the state, and some gates allowing or denying some types of access to such cell:\n",
    "\n",
    "\n",
    "<img src=\"img/LSTM.png\" width=\"500\">\n",
    "\n",
    "Basically, this unit learns to build signals such as $i_t$, $o_t$, $f_t$ $\\in (0,1)$, and adjust the processes of writing, reading and resetting the information the cell holds through the following math:\n",
    "* $i_t = \\sigma(\\theta_{xi}X_t + \\theta_{hi}h_{t-1} + b_i)$\n",
    "* $f_t = \\sigma(\\theta_{xf}X_t + \\theta_{hf}h_{t-1} + b_f)$\n",
    "* $o_t = \\sigma(\\theta_{xo}X_t + \\theta_{ho}h_{t-1} + b_o)$\n",
    "* $g_t = tanh(\\theta_{xg}X_t + \\theta_{hg}h_{t-1} + b_g)$\n",
    "* $c_t = f_t \\otimes c_{t-1}+i_t \\otimes g_t$\n",
    "* $h_t = o_t \\otimes tanh(c_t)$\n",
    "\n",
    "where $\\otimes$ represents element-wise multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start practice\n",
    "Today's task is called **sentiment analysis**. We will try to classify tweets into two categories: `positive` vs `negative`.\n",
    "\n",
    "As this task involves natural language processing, we need to model the sequence of words of each tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this purpose, we provide two different files, namely `data/tweets_train.csv` and `data/tweets_train.csv`. Each file contains a list of tweets along with their corresponding label. In order to encode a word into a numerical vector, we need to do the following:\n",
    "* create a *dictionary* holding all the words contained in the training data. It is common to clip this structure to a fixed dimension (e.g. keeping only the $N$ most common words);\n",
    "* represent each word as a $N$ dimensional one-hot vector which encodes its position in the *dictionary*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Quadro K2200 (CNMeM is disabled, cuDNN 5005)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Input, Dense, Flatten, MaxPooling1D\n",
    "import re\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "\n",
    "# each tweet is made by max. 140 characters\n",
    "MAX_TWEET_CHARS = 140\n",
    "\n",
    "\n",
    "def preprocess(line):\n",
    "    \"\"\"\n",
    "    Pre-process a string of text. Eventually add additional pre-processing here.\n",
    "    \"\"\"\n",
    "    line = line.lower()               # turn to lowercase\n",
    "    line = line.replace('\\n', '')     # remove newlines\n",
    "    line = re.sub(r'\\W+', ' ', line)  # keep characters only (\\W is short for [^\\w])\n",
    "\n",
    "    return line\n",
    "\n",
    "\n",
    "def get_dictionary(filename, dict_size=2000):\n",
    "    \"\"\"\n",
    "    Read the tweets and return a list of the 'max_words' most common words.\n",
    "    \"\"\"\n",
    "    all_words = []\n",
    "    with open(filename, 'rb') as csvfile:\n",
    "        r = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for row in r:\n",
    "            tweet = row[3]\n",
    "            if len(tweet) <= MAX_TWEET_CHARS:\n",
    "                words = preprocess(tweet).split()\n",
    "                all_words += words\n",
    "\n",
    "    word_counter = Counter(all_words)\n",
    "    dictionary, _ = zip(*word_counter.most_common(min(dict_size, len(word_counter))))\n",
    "\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our training data will have size `(n_examples, sequence_length, N)`.\n",
    "\n",
    "Since the size of such structure may be very large, we use a **generator** to load data batches only when needed during the training stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(filename, batchsize, maxlen=50, dict_size=2000):\n",
    "    \"\"\"\n",
    "    Generate a batch of training data\n",
    "    \"\"\"\n",
    "\n",
    "    # get the list of words that will constitute our dictionary (once only)\n",
    "    dictionary = get_dictionary(filename, dict_size)\n",
    "\n",
    "    # read training data (once only)\n",
    "    rows = []\n",
    "    with open(filename, 'rb') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for row in reader:\n",
    "            rows.append(row)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # prepare data structures\n",
    "        X_batch = np.zeros((batchsize, maxlen, len(dictionary)+1), dtype=np.float32)\n",
    "        Y_batch = np.zeros(batchsize, dtype=np.float32)\n",
    "\n",
    "        for i in range(batchsize):\n",
    "\n",
    "            rand_idx = np.random.randint(0, len(rows))\n",
    "            Y_batch[i] = float(rows[rand_idx][1])\n",
    "\n",
    "            random_tweet = rows[rand_idx][3]\n",
    "            if len(random_tweet) <= MAX_TWEET_CHARS:\n",
    "\n",
    "                words = preprocess(random_tweet).split()\n",
    "\n",
    "                # vectorization\n",
    "                for j, w in enumerate(words):\n",
    "                    try:\n",
    "                        w_idx = dictionary.index(w)\n",
    "                        X_batch[i, j, w_idx + 1] = 1\n",
    "                    except Exception:\n",
    "                        # word not found, using the unknown\n",
    "                        X_batch[i, j, 0] = 1\n",
    "            else:\n",
    "                i -= 1\n",
    "\n",
    "        yield X_batch, Y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To speed things up, we already prepared the skeleton of the `__main__` file. Feel free to try different models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32768/32768 [==============================] - 79s - loss: 0.5337 - val_loss: 0.9518\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    max_len = 50      # max sequence length\n",
    "    dict_size = 1500  # dictionary clipping \n",
    "    batchsize = 128 \n",
    "\n",
    "    model_in = Input(shape=(max_len, dict_size + 1))\n",
    "    # ...\n",
    "    # your model goes here\n",
    "    # ...\n",
    "       \n",
    "    model_out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(input=model_in, output=model_out)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "    model.fit_generator(generate_batch('data/tweets_train.csv', batchsize, maxlen=max_len, dict_size=dict_size),\n",
    "                        nb_epoch=1,\n",
    "                        samples_per_epoch=256*batchsize,\n",
    "                        validation_data=generate_batch('data/tweets_val.csv', batchsize, maxlen=max_len, dict_size=dict_size),\n",
    "                        nb_val_samples=100*batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model:\n",
    "When training ends, you can test the model by classifying your own tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST phase: ------------------------------------------------------------\n",
      "Write your own tweet (max 50 words):\n",
      "what a wonderful morning!\n",
      "Model output: [[ 0.81711435]]\n",
      ":)\n",
      "Write your own tweet (max 50 words):\n",
      "i want to die asap\n",
      "Model output: [[ 0.0594591]]\n",
      ":(\n"
     ]
    }
   ],
   "source": [
    "    print('TEST phase: ' + 60*'-')\n",
    "\n",
    "    dictionary = get_dictionary('data/tweets_train.csv', dict_size)\n",
    "\n",
    "    while True:\n",
    "        my_tweet = raw_input('Write your own tweet (max {} words):\\n'.format(max_len)).lower()\n",
    "        if my_tweet:\n",
    "            words = preprocess(my_tweet).split()\n",
    "            X_test = np.zeros((1, max_len, dict_size + 1))\n",
    "            # vectorization\n",
    "            for j, w in enumerate(words):\n",
    "                try:\n",
    "                    w_idx = dictionary.index(w)\n",
    "                    X_test[0, j, w_idx + 1] = 1\n",
    "                except Exception:\n",
    "                    # word not found, using the unknown\n",
    "                    X_test[0, j, 0] = 1\n",
    "\n",
    "            Y_test = model.predict(X_test)\n",
    "            print('Model output: {}'.format(Y_test))\n",
    "            print(':)' if Y_test > 0.5 else ':(')\n",
    "        else:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
