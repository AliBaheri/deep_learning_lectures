{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Theano\n",
    "In this tutorial we will move the first steps in the [Theano](http://deeplearning.net/software/theano/) library, a popular tensor manipulation library that provides automatic differentiation. We will learn the basics implementing a simple linear regression solver. \n",
    "We will test both the univariate and multivariate case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember linear regression?\n",
    "\n",
    "Recall that given a dataset $\\{(x_i, y_i)\\}_{i=0}^N$, with $x_i, y_i \\in \\mathbb{R}$, the objective of the univariate linear regression is to find two scalars $w$ and $b$ such that $y = w\\cdot x + b$ fits the dataset. In this tutorial we will learn $w$ and $b$ using SGD and a Mean Square Error (MSE) loss:\n",
    "\n",
    "$$L = \\frac{1}{N} \\sum_{i=0}^N (w\\cdot x_i + b - y_i)^2$$\n",
    "\n",
    "Starting from random values, parameters $w$ and $b$ will be updated at each iteration via the following rule:\n",
    "\n",
    "$$w_t = w_{t-1} - \\eta \\frac{\\partial L}{\\partial w}$$\n",
    "$$b_t = b_{t-1} - \\eta \\frac{\\partial L}{\\partial b}$$\n",
    "\n",
    "where $\\eta$ represents the learning rate of our optimizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the multivariate case different?\n",
    "Not really. The only meaningful thing is that $x_i$ is no more a scalar, but a point in $\\mathbb{R}^d$. Thus, $w$ is also a $d$-dimensional vector, and the linear model is expressed in terms of the dot product between the two: $y = w^T x + b$.\n",
    "\n",
    "Of course, the formula for MSE changes accordingly:\n",
    "\n",
    "$$L = \\frac{1}{N} \\sum_{i=0}^N (w^Tx_i + b - y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Ok, we need some data to fit. We will test both the univariate and the multivariate regression with a \"simple/synthetic\" and \"hard/real\" dataset respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your own synthetic dataset\n",
    "For the univariate case, we can create our simple function that will generate observations and targets. Such function will receive in input the desired number of training and validation examples, as well as two scalars $a$ and $b$, to generate random  $(x,y)$ couples such that $y=a\\cdot x +b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Training samples\n",
      "2000 Validation samples\n",
      "1 observed variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Quadro K2200 (CNMeM is disabled, cuDNN 5103)\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_univariate_data(nb_train, nb_val, a=2, b=1):\n",
    "    X_train = np.random.rand(nb_train, 1)\n",
    "    X_val = np.random.rand(nb_val, 1)\n",
    "\n",
    "    Y_train = a*X_train+b\n",
    "    Y_val = a*X_val+b\n",
    "\n",
    "    return X_train, np.squeeze(Y_train), X_val, np.squeeze(Y_val)\n",
    "\n",
    "# Load data\n",
    "X_train, Y_train, X_val, Y_val = get_univariate_data(10000,2000)\n",
    "n_feat = X_train.shape[1]\n",
    "\n",
    "print '{} Training samples'.format(X_train.shape[0])\n",
    "print '{} Validation samples'.format(X_val.shape[0])\n",
    "print '{} observed variables'.format(n_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A real problem to solve\n",
    "To test the multivariate regression, we will use data from the [House Pricing Kaggle Competition](http://www.kaggle.com/c/house-prices-advanced-regression-techniques/). The proposed challenge is to predict a house price given other 79 variables, describing various aspects of it. Notice that this problem is hard and highly non-linear but, for the sake of this tutorial, we will use only real-valued features (about 30 variables) and a linear model.\n",
    "The function we will use to load data will\n",
    "* load data from the csv file into a numpy array;\n",
    "* remove columns containing non numerical or incomplete data;\n",
    "* shuffle the entire set\n",
    "* separate observations from targets\n",
    "* optionally add artificial quadratic features (simply as product of the existing ones)\n",
    "* split into training and validation samples\n",
    "\n",
    "and will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1168L, 33L)\n",
      "1168 Training samples\n",
      "292 Validation samples\n",
      "33 observed variables\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_multivariate_data(non_linearities=False, train_ratio=0.8):\n",
    "\n",
    "    # Load data\n",
    "    data = np.genfromtxt('data/regression.csv', delimiter=',').astype(np.float32)\n",
    "    data = data[:, ~np.isnan(data).any(axis=0)]\n",
    "\n",
    "    # Shuffle data\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    # separate into observations and targets\n",
    "    X = data[:, 1:-1]\n",
    "    Y = data[:, -1]\n",
    "\n",
    "    # Optionally add non linear features\n",
    "    if non_linearities:\n",
    "        X_cl = X.copy()\n",
    "        n_var = X.shape[1]\n",
    "        for i in range(0, n_var):\n",
    "            for j in range(i, n_var):\n",
    "                f = np.expand_dims(X_cl[:, i]*X_cl[:, j], axis=1)\n",
    "                if f.max() != 0:  \n",
    "                    X = np.concatenate((X, f), axis=1)\n",
    "\n",
    "    # normalize X\n",
    "    x_norm = np.max(X, axis=0)\n",
    "    X /= x_norm\n",
    "\n",
    "    # Split into train and validation\n",
    "    X_train = X[0:int(X.shape[0]*train_ratio), :]\n",
    "    Y_train = Y[0:int(Y.shape[0]*train_ratio)]\n",
    "    X_val = X[int(X.shape[0]*train_ratio)::, :]\n",
    "    Y_val = Y[int(Y.shape[0]*train_ratio)::]\n",
    "\n",
    "    return X_train, Y_train, X_val, Y_val\n",
    "\n",
    "# Load data\n",
    "X_train, Y_train, X_val, Y_val = get_multivariate_data()\n",
    "n_feat = X_train.shape[1]\n",
    "\n",
    "print X_train.shape\n",
    "print '{} Training samples'.format(X_train.shape[0])\n",
    "print '{} Validation samples'.format(X_val.shape[0])\n",
    "print '{} observed variables'.format(n_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start!\n",
    "### Placeholders and variables\n",
    "To implement and run a linear model, we will use the [Keras backend module](http://keras.io/backend/), which provides an abstraction over Theano.\n",
    "\n",
    "First of all, we define the necessary variables and placeholders for our computational graph. Variables maintain state across executions of the computational graph, while placeholders are ways to feed the graph with external data.\n",
    "\n",
    "For the linear regression example, we need three variables: `w`, `b`, and the learning rate for SGD, `lr`. Two placeholders `x` and `target` are created to store $x_i$ and $y_i$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Placeholders and variables\n",
    "x = K.placeholder(shape=(None, n_feat))\n",
    "target = K.placeholder(shape=(1,))\n",
    "lr = K.variable(0.01)\n",
    "w = K.variable(np.random.rand(n_feat))\n",
    "b = K.variable(np.random.rand())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition\n",
    "Now we can define the $y = w^Tx + b$ relation as well as the MSE loss in the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define model and loss\n",
    "y = K.dot(x, w) + b\n",
    "loss = K.mean(K.square(y-target))\n",
    "eval_loss = K.function(inputs=[x, target], outputs=[loss])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, given the gradient of MSE wrt to `w` and `b`, we can define how we update the parameters via SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradients\n",
    "grads = K.gradients(loss, [w, b])\n",
    "updates = [[w, w-lr*grads[0]], [b, b-lr*grads[1]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole model can be encapsulated in a `function`, which takes as input `x` and `target`, returns the current loss value and updates its parameter according to `updates`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "train = K.function(inputs=[x, target], outputs=[loss], updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Training is now just a matter of calling the `function` we have just defined. Each time `train` is called, indeed, `w` and `b` will be updated using the SGD rule.\n",
    "\n",
    "We define a number of epochs at each of wich our dataset will be scanned. We can monitor both the loss on training and validation sets keeping a list, and print them and plot them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4845862400.000 \t Validation loss: 4430183424.000\n",
      "Training loss: 4498284544.000 \t Validation loss: 4186102016.000\n",
      "Training loss: 4295406080.000 \t Validation loss: 4010717440.000\n",
      "Training loss: 4108082944.000 \t Validation loss: 3846859008.000\n",
      "Training loss: 3934639616.000 \t Validation loss: 3695706624.000\n",
      "Training loss: 3773977600.000 \t Validation loss: 3556403200.000\n",
      "Training loss: 3625091584.000 \t Validation loss: 3428019200.000\n",
      "Training loss: 3487056896.000 \t Validation loss: 3309687808.000\n",
      "Training loss: 3359023872.000 \t Validation loss: 3200614144.000\n",
      "Training loss: 3240212992.000 \t Validation loss: 3100066048.000\n",
      "Training loss: 3129907456.000 \t Validation loss: 3007370752.000\n",
      "Training loss: 3027447296.000 \t Validation loss: 2921910016.000\n",
      "Training loss: 2932227328.000 \t Validation loss: 2843114496.000\n",
      "Training loss: 2843689728.000 \t Validation loss: 2770460672.000\n",
      "Training loss: 2761321984.000 \t Validation loss: 2703467264.000\n",
      "Training loss: 2684653056.000 \t Validation loss: 2641690624.000\n",
      "Training loss: 2613249024.000 \t Validation loss: 2584723200.000\n",
      "Training loss: 2546709760.000 \t Validation loss: 2532189952.000\n",
      "Training loss: 2484668672.000 \t Validation loss: 2483744768.000\n",
      "Training loss: 2426786304.000 \t Validation loss: 2439069440.000\n",
      "Training loss: 2372750592.000 \t Validation loss: 2397871104.000\n",
      "Training loss: 2322275328.000 \t Validation loss: 2359880448.000\n",
      "Training loss: 2275095296.000 \t Validation loss: 2324847872.000\n",
      "Training loss: 2230966272.000 \t Validation loss: 2292544768.000\n",
      "Training loss: 2189663488.000 \t Validation loss: 2262760448.000\n",
      "Training loss: 2150979072.000 \t Validation loss: 2235299584.000\n",
      "Training loss: 2114722176.000 \t Validation loss: 2209983488.000\n",
      "Training loss: 2080715648.000 \t Validation loss: 2186646784.000\n",
      "Training loss: 2048797440.000 \t Validation loss: 2165136896.000\n",
      "Training loss: 2018816256.000 \t Validation loss: 2145312896.000\n",
      "Training loss: 1990632832.000 \t Validation loss: 2127046272.000\n",
      "Training loss: 1964119552.000 \t Validation loss: 2110216832.000\n",
      "Training loss: 1939157760.000 \t Validation loss: 2094713984.000\n",
      "Training loss: 1915637760.000 \t Validation loss: 2080436096.000\n",
      "Training loss: 1893458304.000 \t Validation loss: 2067289600.000\n",
      "Training loss: 1872525184.000 \t Validation loss: 2055187456.000\n",
      "Training loss: 1852752128.000 \t Validation loss: 2044049920.000\n",
      "Training loss: 1834059264.000 \t Validation loss: 2033803392.000\n",
      "Training loss: 1816371968.000 \t Validation loss: 2024379008.000\n",
      "Training loss: 1799621632.000 \t Validation loss: 2015714560.000\n",
      "Training loss: 1783744512.000 \t Validation loss: 2007751040.000\n",
      "Training loss: 1768681984.000 \t Validation loss: 2000435200.000\n",
      "Training loss: 1754378624.000 \t Validation loss: 1993718272.000\n",
      "Training loss: 1740784512.000 \t Validation loss: 1987552896.000\n",
      "Training loss: 1727852544.000 \t Validation loss: 1981897984.000\n",
      "Training loss: 1715539072.000 \t Validation loss: 1976713728.000\n",
      "Training loss: 1703803520.000 \t Validation loss: 1971964288.000\n",
      "Training loss: 1692608384.000 \t Validation loss: 1967616256.000\n",
      "Training loss: 1681919232.000 \t Validation loss: 1963638528.000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhIAAAFyCAYAAACgITN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZExBQkRERDL2/wHAbrV1/11BuwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18cfb3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs):\n",
    "    train_loss = train([X_train, Y_train])[0]\n",
    "    train_loss_history.append(train_loss)\n",
    "\n",
    "    val_loss = eval_loss([X_val, Y_val])[0]\n",
    "    val_loss_history.append(val_loss)\n",
    "\n",
    "    # Print and plot\n",
    "    if epoch % 20 == 0 and epoch != 0:\n",
    "        print \"Training loss: %.03f \\t Validation loss: %.03f\" % (train_loss, val_loss)\n",
    "\n",
    "        # Plot loss history\n",
    "        tr, = plt.plot(train_loss_history, c='r')\n",
    "        vl, = plt.plot(val_loss_history, c='b')\n",
    "        plt.legend([tr, vl], ['Train', 'Val'])\n",
    "        plt.xlim(0, nb_epochs)\n",
    "        plt.ylim(0, max(max(train_loss_history[3:]), max(val_loss_history[3:])))\n",
    "        plt.title('[Epoch: {}] Loss history'.format(epoch))\n",
    "            \n",
    "        plt.draw()\n",
    "        plt.pause(0.001)\n",
    "\n",
    "# Final plot (still)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra\n",
    "Sometimes when training on the multivariate dataset augmented with non linear features may fall in overfitting. Can you spot it from the loss function behaviour? How can you adjust the loss function in order to reduce this effect?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
