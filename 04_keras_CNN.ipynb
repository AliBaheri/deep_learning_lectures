{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "In this tutorial we will see how to define, train and visualize a Convolutional Neural Network (CNN) for image classification using the [Keras](https://keras.io/) library.\n",
    "\n",
    "## From theory...\n",
    "\n",
    "Recall (see [here](http://cs231n.github.io/convolutional-networks/) for details):\n",
    "\n",
    "> Convolutional Neural Networks are very similar to ordinary Neural Networks from the previous chapter: they are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity. So what does change? ConvNet architectures make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture. These then make the forward function more efficient to implement and vastly reduce the amount of parameters in the network.\n",
    "\n",
    "Most CNNs have a feed-forward architecture and are tipically composed of [Convolutional](https://keras.io/layers/convolutional/), [Pooling](https://keras.io/layers/pooling/) and [Fully-Connected](https://keras.io/layers/core/#dense) layers.\n",
    "\n",
    "CNN models first demonstrated their effectiveness for task of classification. In this context, a [Softmax](https://keras.io/activations/) activation after the last Dense layer encodes the probability distribution over the classes and the loss function employed is the categorical [cross-entropy](https://keras.io/objectives/).\n",
    "\n",
    "More recently CNNs have been successfully employed to tackle a huge variety of problems, including e.g. semantic segmentation, visual attention, edge detection, image and video captioning etc. (see e.g. [here](https://github.com/kjw0612/awesome-deep-vision) for some examples). In order to deal with these different tasks, network architectures have evolved too from simple feed-forward to arbitrarily complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ...to practice.\n",
    "\n",
    "[Keras](https://keras.io/) library provides facility for the [feed-forward](https://keras.io/models/sequential/) as well as for [more complex](https://keras.io/getting-started/functional-api-guide/) architectures. The latter can be used for defining composite models, such as multi-output models, directed acyclic graphs, or models with shared layers.\n",
    "\n",
    "**In this tutorial, we train a simple feed-forward CNN to perform image classification** on the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset, which consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. For this task, a [sequential](https://keras.io/models/sequential/) model is good enough: however, we'll tincker with the [functional API](https://keras.io/getting-started/functional-api-guide/) in order to acquire confidence with both syntaxes. Let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we just load the CIFAR-10 data and normalize the training and test images to lie in the range $[0, 1]$. The only thing to note here is that for multi-class classification is often useful to convert the target from scalar values $t \\in [0, 1, \\dots, nb_{classes}]$ to *one-hot* vectors. For example, suppose we have $C=10$ classes: the target $t_i=3$, indicating that $x_i$ belongs to the class $3$, is converted to the following *one-hot* vector: $[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]$. Being this operation very common, Keras provides a built-in [function](https://keras.io/utils/np_utils/) to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "Train samples: 50000\n",
      "Test samples: 10000.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# cifar 10 classes\n",
    "nb_classes = 10\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols, img_channels = 32, 32, 3\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize X in range (0, 1)\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Train samples: {}\\nTest samples: {}.'.format(X_train.shape[0], X_test.shape[0]))\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model (sequential API)\n",
    "If we follow the syntax of the sequential API, we need to create an instance of `Sequential()` and then stack all the layers one on the top of the other with `model.add(layer)`. Notice that for the first layer we have to specify the input's shape!\n",
    "\n",
    "For `Convolution2D`, we have to specify the number of learned filters and the kernel size (written in explicit in the first layer). We can optionally specify other parameters, such as the `border_mode=same` not to change the width and height of the input. Remember that the default value `border_mode=valid` will reduce width and height by $k-1$, where $k$ is the size of the kernel. *ReLu* activations follow each convolutional layer and have no parameter.\n",
    "\n",
    "Layers of `MaxPooling2D` take as parameter the height and width of the spatial `pool_size`. By default the pooling `strides` are set equal to the `pool_size`.\n",
    "\n",
    "Layers of `Dropout` take a parameter $p \\in [0,1]$, which represents the probability to set to 0 a certain input unit at each update during trainig time. As you should know, dropout is a form of regularization which helps prevent overfitting.\n",
    "\n",
    "Finally, we have our fully-connected a.k.a. `Dense` layers: the first has 512 outputs, the second has as many outputs as the number of classes (10 in our case). The output of the second `Dense` layer is a vector containing `nb_classes` values in an arbitrary range. The `SoftMax` activation squashes these values to the range $[0,1]$, normalizing them such that they sum to $1$ (the math [here](https://en.wikipedia.org/wiki/Softmax_function)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(nb_filter=32, nb_row=3, nb_col=3, border_mode='same', input_shape=X_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model (functional API)\n",
    "If we follow the functional API, things are slightly different. We have an `Input` instance which represents the input of the network. Each layer can now be applied just as a function to a certain input and produces a certain output. For example, the first `Convolution2D` is applied to `model_in` and produces the output `x`. Using this notation, appropriate layers can be stacked together to obtain the previous architecture. At the end, an instance of `Model` is created and is given the inputs and outputs of the computational graph.\n",
    "\n",
    "In this simple case there's little difference with the sequential version: however, the functional syntax in general guarantees a huge flexibility in the construction of the network architecture. It's advisable to check the guide to the functional API [here](https://keras.io/getting-started/functional-api-guide/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten \n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "\n",
    "model_in = Input(shape=X_train.shape[1:])\n",
    "\n",
    "x = Convolution2D(32, 3, 3, activation='relu', border_mode='same') (model_in)\n",
    "x = Convolution2D(32, 3, 3,  activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "x = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(x)\n",
    "x = Convolution2D(64, 3, 3, activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(nb_classes)(x)\n",
    "model_out = Activation('softmax')(x)\n",
    "\n",
    "model = Model(input=model_in, output=model_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everything is right? Doublecheck the summary!\n",
    "A nice feature of Keras is that we can have access to a text summary of the whole model architecture by simply calling `model.summary()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 32, 32, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 32, 32, 32)    896         input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)  (None, 30, 30, 32)    9248        convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 15, 15, 32)    0           convolution2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 15, 15, 32)    0           maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_7 (Convolution2D)  (None, 15, 15, 64)    18496       dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_8 (Convolution2D)  (None, 13, 13, 64)    36928       convolution2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)    (None, 6, 6, 64)      0           convolution2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 6, 6, 64)      0           maxpooling2d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 2304)          0           dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 512)           1180160     flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 512)           0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 10)            5130        dropout_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 10)            0           dense_4[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 1250858\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, in the summary are reported all information about the computational graph that is going to be compiled. Make sure to understand both the output shape and the number of weights of each layer. Where do the most of the parameters come from?\n",
    "\n",
    "In order to inspect the weights learned by each layer, we can also call the `layer.get_weights()` method. This will return a list with two elements, the first one being the weight tensor and the second one being the bias vector. Recall that convolutional layers learn a $(n_o, n_i, k, k)$ weight tensor, where $k$ is the size of the kernel, $n_i$ is the number of channels of the input tensor, and $n_o$ is the number of filters to be learned. A bias is also learned for each of the $n_o$ filters. Dense layers insted learn a $(n_i, n_o)$ weight tensor, where $n_o$ is the output size and $n_i$ is the input size of the layer. Each of the $n_o$ neurons also has a bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model\n",
    "\n",
    "In order to compile the computational graph, you need to provide at least an optimizer and a loss function. In this case we employ as **optimizer** vanilla SGD with momentum: however, Keras provides a variety of different [bulti-in optimizers](https://keras.io/optimizers/), so feel free to try another one. Being the task multi-class classification, we unsurprisingly choose as **loss** function the [categorical cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "# Let's train the model using SGD + momentum:\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let the training begin!\n",
    "\n",
    "Now that everything is in place, we are ready to actually start the training of the network. First we set some parameters, in particular the **batch size** is the size of the batch used to perform the gradient descent. We can also decide to leverage on Keras [support](https://keras.io/preprocessing/image/) for **data augmentation**, which is always performed in CPU in a separate thread. In this case, we also apply some image transformation like rotations, shifts and flips to the training images, to reduce the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "batch_size = 128\n",
    "nb_epoch = 200\n",
    "data_augmentation = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we can really start. If everything has gone well the loss should gets down to 0.65 (test logloss) in 25 epochs, and down to 0.55 after 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Epoch 1/200\n",
      "30848/50000 [=================>............] - ETA: 9s - loss: 2.3023 - acc: 0.1026"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(X_train, Y_train,\n",
    "              batch_size=batch_size,\n",
    "              nb_epoch=nb_epoch,\n",
    "              validation_data=(X_test, Y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(X_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n",
    "                        samples_per_epoch=X_train.shape[0],\n",
    "                        nb_epoch=nb_epoch,\n",
    "                        validation_data=(X_test, Y_test))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbpresent": {
   "slides": {
    "523a7dae-c51d-4dcf-8e26-1a88d05fa795": {
     "id": "523a7dae-c51d-4dcf-8e26-1a88d05fa795",
     "prev": "c7d07a25-8a15-4d9c-b1d4-a57fe844e2e5",
     "regions": {
      "b5818082-2dce-4a8b-a801-ccd7cb39b83b": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "3528348c-c1b8-4c09-9cb6-4dd5772c2a87",
        "part": "whole"
       },
       "id": "b5818082-2dce-4a8b-a801-ccd7cb39b83b"
      }
     }
    },
    "a0a6c0ee-bb21-4d9f-8886-42a5ebe48df0": {
     "id": "a0a6c0ee-bb21-4d9f-8886-42a5ebe48df0",
     "prev": "523a7dae-c51d-4dcf-8e26-1a88d05fa795",
     "regions": {
      "62d069a1-5662-4680-aa7f-60a2dd6b1a98": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f8c3e7fd-5711-429c-9398-a24c43950197",
        "part": "whole"
       },
       "id": "62d069a1-5662-4680-aa7f-60a2dd6b1a98"
      }
     }
    },
    "c7d07a25-8a15-4d9c-b1d4-a57fe844e2e5": {
     "id": "c7d07a25-8a15-4d9c-b1d4-a57fe844e2e5",
     "prev": "f09912f6-3947-462a-ac87-43abff450e5c",
     "regions": {
      "dd7feb31-2640-4e6c-9bfa-6066fbe4bf87": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "983e7694-97d7-4c99-abed-8cdfb0e78020",
        "part": "whole"
       },
       "id": "dd7feb31-2640-4e6c-9bfa-6066fbe4bf87"
      }
     }
    },
    "f09912f6-3947-462a-ac87-43abff450e5c": {
     "id": "f09912f6-3947-462a-ac87-43abff450e5c",
     "prev": null,
     "regions": {
      "262ac9d3-e3d3-47bc-8b19-ac08ab2e553e": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "4e5bbdc5-00cd-4457-a619-fa315f702739",
        "part": "whole"
       },
       "id": "262ac9d3-e3d3-47bc-8b19-ac08ab2e553e"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
