{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Neuron\n",
    "\n",
    "In this tutorial we will implement a single perceptron in [Keras](https://keras.io/) and we will employ it to perform two-class classification on the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset.\n",
    "\n",
    "## From theory...\n",
    "\n",
    "The output of a *single neuron*, given an input $x$ is the following: $$y = \\sigma(W^Tx + b)$$\n",
    "Where $W$ and $b$ are the learned matrix of weights and the learned bias respectively. The greek letter $\\sigma$ stands for the activation function: in this case we'll use the sigmoid function, that is $\\sigma(z) = 1 / (1 + e^{-z})$.\n",
    "\n",
    "Being the task binary-classification, we'll use as loss *binary cross-entropy*, defined as: \n",
    "$$L(t, p) = -tlog(p) -(1-t)log(1-p)$$\n",
    "where $t \\in \\left\\{0, 1\\right\\}$ is the target and $p \\in (0, 1)$ is our prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ...to practice\n",
    "\n",
    "For this task we'll use the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset, which consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. The whole dataset is quite small (~160MB) and the [Keras](https://keras.io/) library provides a facility for downloading and unpacking the dataset (if not already in cache), as well as for splitting training and testing examples. This is as easy as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Quadro K2200 (CNMeM is disabled, cuDNN 5005)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train shape:', (50000L, 3L, 32L, 32L))\n",
      "Train samples: 50000\n",
      "Test samples: 10000.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "\n",
    "(X_train, Y_train), (X_val, Y_val) = cifar10.load_data()\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Train samples: {}\\nTest samples: {}.'.format(X_train.shape[0], X_val.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single neuron is able to distinguish only between two classes. Thus, we already provide the code to filter the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset in order to get only a couple of classes. Here is the list of all the ten classes:\n",
    "> `airplane`, `automobile`, `bird`, `cat`, `deer`, `dog`, `frog`, `horse`, `ship`, `truck`\n",
    "\n",
    "From these, we can select a pair of classes (say  *horse* and *truck*) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train shape:', (10000L, 3072L))\n",
      "Train samples: 10000\n",
      "Test samples: 2000.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "\n",
    "# size of cifar10 images\n",
    "height, width, channels = 32, 32, 3  \n",
    "\n",
    "\n",
    "def select_classes_from_cifar10(dataset, classes):\n",
    "\n",
    "    allowed = 'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "    if any([not cl in classes for cl in classes]) or not len(classes) > 1:\n",
    "        raise ValueError('Please check your class arguments')\n",
    "\n",
    "    (X_train, Y_train), (X_val, Y_val) = dataset\n",
    "\n",
    "    c_idx = [allowed.index(c) for c in classes]  # class indexes\n",
    "\n",
    "    # find the ids of the right examples\n",
    "    idx_train = np.array([True if y[0] in c_idx else False for y in Y_train])\n",
    "    idx_val = np.array([True if y[0] in c_idx else False for y in Y_val])\n",
    "\n",
    "    # filter and reshape X\n",
    "    X_train = np.float32(X_train[idx_train])\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], height * width * channels))\n",
    "    X_val = np.float32(X_val[idx_val])\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], height * width * channels))\n",
    "\n",
    "    # adjust Y\n",
    "    s = sorted(c_idx)\n",
    "    Y_train = np.float32([s.index(y[0]) for y in Y_train[idx_train]])\n",
    "    Y_val = np.float32([s.index(y[0]) for y in Y_val[idx_val]])\n",
    "\n",
    "    return (X_train/X_train.max(), Y_train), (X_val/X_train.max(), Y_val)\n",
    "\n",
    "\n",
    "# load data from cifar\n",
    "cifar10_dataset = cifar10.load_data()\n",
    "(X_train, Y_train), (X_val, Y_val) = select_classes_from_cifar10(cifar10_dataset, classes=['horse', 'truck'])\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Train samples: {}\\nTest samples: {}.'.format(X_train.shape[0], X_val.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our two-class dataset, containing only *horses* and *trucks*! Let's go on defining our model.\n",
    "\n",
    "### Placeholders and Variables \n",
    "\n",
    "First we'll need some placeholders in order to be able to feed our computational graph with external data, such as images and targets. These can be defined through `K.placeholder(shape=())`. In our case *x* is a vector with the same shape of the input image, while the *target* is a scalar $\\in \\left\\{0,1\\right\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# placeholders variables\n",
    "x = K.placeholder(shape=(None, height * width * channels))\n",
    "target = K.placeholder(shape=(1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the variables: on these, `keras.backend` will compute the automatic derivative during the SGD.\n",
    "\n",
    "We need a vector *w* with the same shape of *x*, and a scalar bias *b*. Both variables are initialized with random normal weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variables\n",
    "w = K.variable(np.random.randn(height * width * channels))\n",
    "b = K.variable(np.random.randn())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model\n",
    "\n",
    "We can define the neuron output $y = \\sigma(W^Tx + b)$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = K.sigmoid(K.dot(x, w) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to define our **loss**, which as we saw is the binary cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss = K.mean(K.binary_crossentropy(output=y, target=target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a couple of utility functions that will be useful to get a feeling of what's happening during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility functions\n",
    "compute_loss = K.function(inputs=[x, target], outputs=[loss])\n",
    "predict = K.function(inputs=[x], outputs=[y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have to tell to Theano to compute the **gradients** of our variables *w* and *b* with respect to our loss. Let's do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grads = K.gradients(loss=loss, variables=[w, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once evaluated, `grads[0]` will contain the gradient of the loss w.r.t. the weights *w*, `grads[1]` will contain the gradient of the loss w.r.t. the bias *b*.\n",
    "\n",
    "We can now set the **learning rate** and define the **update rule**: $w_{t+1} = w_t - lr~\\frac{\\partial L}{\\partial w}$ and $b_{t+1} = b_t - lr~\\frac{\\partial L}{\\partial b}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update rule\n",
    "lr = K.variable(0.1)\n",
    "updates = [[w, w - lr * grads[0]], [b, b - lr * grads[1]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the actual training function is defined. We have to provide the inputs (that will feed the placeholders), the outputs and the parameters' update. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train function\n",
    "train = K.function(inputs=[x, target], outputs=[loss], updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to start the **training**. For `nb_epochs`, the `train` function is called and parameters are updated with the update rule previously defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training\n",
    "nb_epochs = 1000\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "plt.figure()\n",
    "for epoch in range(nb_epochs):\n",
    "    # calc loss and update parameters\n",
    "    loss_train = train([X_train, Y_train])[0]\n",
    "    train_loss_history.append(loss_train)\n",
    "\n",
    "    # just calc loss for validation\n",
    "    loss_val = compute_loss([X_val, Y_val])[0]\n",
    "    val_loss_history.append(loss_val)\n",
    "\n",
    "    # plot and print\n",
    "    if epoch % 10 == 0:\n",
    "        cur_w = np.reshape(np.array(K.eval(w)), (channels, height, width)).transpose(1, 2, 0)\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(cur_w/cur_w.max(), interpolation='none')\n",
    "        plt.title('[Epoch: {}] Current weights'.format(epoch))\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        tr, = plt.plot(train_loss_history, c='r')\n",
    "        vl, = plt.plot(val_loss_history, c='b')\n",
    "        plt.legend([tr, vl], ['Train', 'Val'])\n",
    "        plt.xlim(0, nb_epochs)\n",
    "        plt.ylim(0, max(max(train_loss_history), max(val_loss_history)))\n",
    "        plt.title('[Epoch: {}] Loss history'.format(epoch))\n",
    "\n",
    "        plt.draw()\n",
    "        plt.pause(0.001)\n",
    "\n",
    "        print 'Training loss: {}\\tValidation loss:{}'.format(loss_train, loss_val)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the neuron learned anything? Let's find out by calling the **`predict`** function on some samples of the validation set `X_val`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "while True:\n",
    "    r = np.random.randint(0, X_val.shape[0])\n",
    "    sample = X_val[r]\n",
    "    prediction = round(predict([np.expand_dims(sample, axis=0)])[0])\n",
    "    sample = np.reshape(sample, (channels, height, width)).transpose(1, 2, 0)\n",
    "\n",
    "    plt.imshow(sample, interpolation='none')\n",
    "    plt.title('Prediction: {}, Label:{}'.format(prediction, Y_val[r]))\n",
    "    plt.draw()\n",
    "    plt.waitforbuttonpress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra\n",
    "\n",
    "- Add a Keras optimizer of your choice to replace vanilla SGD. Training curve should be smoother and loss should get to lower values\n",
    "- Try different initialization for the variables\n",
    "- (spare time?) Try implementing a simple MultiLayer Perceptron in Theano"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
