{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Challenge!\n",
    "\n",
    "\n",
    "In the previous tutorials, we've explored the possibilities given both [Theano](http://deeplearning.net/software/theano/) and [Keras](https://keras.io/) frameworks. For these we provided full-working examples (conditioned on the correct installation of the libraries) which you find in this same repository. *Now it's time to get your hands dirty.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "A lot of machine-learning algorithms are **data-hungry** and the recent advent of deep learning has worsened this situation. The more complex our model, the more data we need to properly train it. However, the truth is that **labeling data is expensive** and more often than not we find ourselves in situations in which we just don't have as much labeled data as we'd like to have.\n",
    "\n",
    "Nonetheless, the good news is that we almost always have a lot of *unlabeled* data. Maybe we can **learn the data structure in an unsupervised way**, then use this information to effectively train our model of the few labeled examples we have.\n",
    "\n",
    "--- \n",
    "\n",
    "The task of this challenge is again multi-class classification on [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset, but with a *caveat*. This time, the **training set is restricted to 500 images** only. However, the other **59K images without label** are also available.\n",
    "\n",
    "### Goal\n",
    "\n",
    "The goal is to obtain the best possible *accuracy* on the test set (also 500 images). You can freely choose your classification model, either *deep* or *shallow*, whatever works best.\n",
    "\n",
    "*NB: If you don't finish in this lab lecture, you can always finish \"offline\" and submit us your solution later.*\n",
    "\n",
    "### Constraints\n",
    "\n",
    "- Supervised training can only be performed on the provided set `X_labeled_train`.\n",
    "- Results can be easily and consistently reproduced\n",
    "\n",
    "That's all. **Good Luck!**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading challenge data\n",
    "\n",
    "In order to split the data appropriately for the challenge, we prepared the `get_unlabeled_cifar10` helper function, (use should be self-explaining)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Quadro K2200 (CNMeM is disabled, cuDNN 5005)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of UNLABELED examples: (Train: 50000, Val: 9000)\n",
      "Number of LABELED examples (Train: 500, Test: 500)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "cifar10_classes = 10\n",
    "\n",
    "\n",
    "def get_unlabeled_cifar10(nb_labeled_examples=1000):\n",
    "\n",
    "    # The data, shuffled and split between train and test sets:\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "    X_unlabeled_train = np.float32(X_train) / 255\n",
    "    X_unlabeled_val = np.float32(X_test[:-nb_labeled_examples]) / 255\n",
    "\n",
    "    X_labeled_train = np.float32(X_test[-nb_labeled_examples:-nb_labeled_examples/2]) / 255\n",
    "    X_labeled_test = np.float32(X_test[-nb_labeled_examples/2:]) / 255\n",
    "\n",
    "    y_labeled_train = y_test[-nb_labeled_examples:-nb_labeled_examples/2]\n",
    "    y_labeled_test = y_test[-nb_labeled_examples/2:]\n",
    "\n",
    "    # Convert class vectors to binary class matrices.\n",
    "    Y_labeled_train = np_utils.to_categorical(y_labeled_train, cifar10_classes)\n",
    "    Y_labeled_test = np_utils.to_categorical(y_labeled_test, cifar10_classes)\n",
    "\n",
    "    return (X_unlabeled_train, X_unlabeled_val), (X_labeled_train, Y_labeled_train), (X_labeled_test, Y_labeled_test)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    (X_unlabel_train, X_unlabel_val), (X_label_train, Y_label_train), (X_label_test, Y_label_test) = get_unlabeled_cifar10()\n",
    "    \n",
    "    print('Number of UNLABELED examples: (Train: {}, Val: {})'.format(len(X_unlabel_train), len(X_unlabel_val)))\n",
    "    print('Number of LABELED examples (Train: {}, Test: {})'.format(len(X_label_train), len(X_label_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other functions that may help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the activation of a layer in a Keras model\n",
    "\n",
    "Let's see how to access the activation of any layer of a [Keras](https://keras.io/) model.\n",
    "\n",
    "First we can define a model `my_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 3, 32, 32)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 32, 32, 32)    896         input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 32, 16, 16)    0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 64, 16, 16)    18496       maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 64, 4, 4)      0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 128, 4, 4)     73856       maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 2048)          0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           262272      flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 128)           16512       dense_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 372032\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from keras.layers import Convolution2D, MaxPooling2D, UpSampling2D, Reshape\n",
    "from keras.models import Model\n",
    "\n",
    "model_input = Input(shape=(3, 32, 32))\n",
    "x = Convolution2D(32, 3, 3, border_mode='same', activation='relu')(model_input)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Convolution2D(64, 3, 3, border_mode='same', activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(4, 4))(x)\n",
    "x = Convolution2D(128, 3, 3, border_mode='same', activation='relu')(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "model_output = Dense(128, activation='relu')(x)\n",
    "my_model = Model(input=model_input, output=model_output)\n",
    "\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want *e.g.* the output of the second Convolutional layer, we see that we find it at position 5 in the list `my_model.layers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: <keras.engine.topology.InputLayer object at 0x0000000004539908>\n",
      "1: <keras.layers.convolutional.Convolution2D object at 0x0000000004539898>\n",
      "2: <keras.layers.pooling.MaxPooling2D object at 0x00000000179B2F28>\n",
      "3: <keras.layers.convolutional.Convolution2D object at 0x0000000017B177B8>\n",
      "4: <keras.layers.pooling.MaxPooling2D object at 0x0000000017BEED30>\n",
      "5: <keras.layers.convolutional.Convolution2D object at 0x0000000017C01F60>\n",
      "6: <keras.layers.core.Flatten object at 0x0000000017BFC438>\n",
      "7: <keras.layers.core.Dense object at 0x0000000017C73CC0>\n",
      "8: <keras.layers.core.Dense object at 0x0000000017D514A8>\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "for (i, layer) in enumerate(my_model.layers):\n",
    "    print('{}: {}'.format(i, layer))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can define a `backend` function to get the output of *that* layer, given a certain input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "which_layer = 5\n",
    "\n",
    "get_activations = K.function([my_model.layers[0].input, K.learning_phase()], my_model.layers[which_layer].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now call `get_activations` with any input `test_example_i` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAREAAAEACAYAAACUHkKwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x183d7e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's take just a random example\n",
    "example_i = np.expand_dims(X_unlabel_train[0], 0)\n",
    "\n",
    "# get the activation of second convolutional layer for this example\n",
    "activation_i = get_activations([example_i, 0])\n",
    "\n",
    "# show activation\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(np.squeeze(activation_i)[0], interpolation='none')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
