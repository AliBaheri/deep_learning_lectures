{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "In this tutorial we will see how to define, train and visualize a Convolutional Neural Network (CNN) for image classification using the [Keras](https://keras.io/) library.\n",
    "\n",
    "## From theory...\n",
    "\n",
    "Recall (see [here](http://cs231n.github.io/convolutional-networks/) for details):\n",
    "\n",
    "> Convolutional Neural Networks are very similar to ordinary Neural Networks from the previous chapter: they are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity. So what does change? ConvNet architectures make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture. These then make the forward function more efficient to implement and vastly reduce the amount of parameters in the network.\n",
    "\n",
    "Most CNNs have a feed-forward architecture and are tipically composed of [Convolutional](https://keras.io/layers/convolutional/), [Pooling](https://keras.io/layers/pooling/) and [Fully-Connected](https://keras.io/layers/core/#dense) layers.\n",
    "\n",
    "CNN models first demonstrated their effectiveness for task of classification. In this context, a [Softmax](https://keras.io/activations/) activation after the last Dense layer encodes the probability distribution over the classes and the loss function employed is the categorical [cross-entropy](https://keras.io/objectives/).\n",
    "\n",
    "More recently CNNs have been successfully employed to tackle a huge variety of problems, including e.g. semantic segmentation, visual attention, edge detection, image and video captioning etc. (see e.g. [here](https://github.com/kjw0612/awesome-deep-vision) for some examples). In order to deal with these different tasks, network architectures have evolved too from simple feed-forward to arbitrarily complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ...to practice.\n",
    "\n",
    "[Keras](https://keras.io/) library provides facility for both the [feed-forward](https://keras.io/models/sequential/) and for [more complex](https://keras.io/getting-started/functional-api-guide/) architectures. The latter can be used for defining composite models, such as multi-output models, directed acyclic graphs, or models with shared layers.\n",
    "\n",
    "**In this tutorial, we train a simple feed-forward CNN to perform image classification** on the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset, which consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. For this task, a [sequential](https://keras.io/models/sequential/) model is good enough: however, we'll tincker with the [functional API](https://keras.io/getting-started/functional-api-guide/) in order to acquire confidence with both syntaxes. Let's start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# cifar 10 classes\n",
    "nb_classes = 10\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols, img_channels = 32, 32, 3\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize X in range (0, 1)\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Train samples: {}    Test samples{}.'.format(X_train.shape[0], X_test.shape[0])\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model (sequential API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3, border_mode='same', input_shape=X_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model (functional API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten \n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "\n",
    "model_in = Input(shape=X_train.shape[1:])\n",
    "\n",
    "x = Convolution2D(32, 3, 3, activation='relu', border_mode='same') (model_in)\n",
    "x = Convolution2D(32, 3, 3,  activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "x = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(x)\n",
    "x = Convolution2D(64, 3, 3, activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(nb_classes)(x)\n",
    "model_out = Activation('softmax')(x)\n",
    "\n",
    "model = Model(input=model_in, output=model_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model\n",
    "\n",
    "Keras provides a variety of different [optimizers](https://keras.io/optimizers/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "# Let's train the model using SGD + momentum:\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let the training begin!\n",
    "\n",
    "Now we are ready to actually start the training of the network. First we set some parameters, in particular the **batch size** is the size of the batch used to perform the gradient descent. We can also decide to leverage on Keras [support](https://keras.io/preprocessing/image/) for **data augmentation**, which is always performed in CPU in a separate thread. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "batch_size = 128\n",
    "nb_epoch = 200\n",
    "data_augmentation = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we can really start. If everything has gone well the loss should gets down to 0.65 (test logloss) in 25 epochs, and down to 0.55 after 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_augmentation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b15c580d831e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata_augmentation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Not using data augmentation.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     model.fit(X_train, Y_train,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_augmentation' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(X_train, Y_train,\n",
    "              batch_size=batch_size,\n",
    "              nb_epoch=nb_epoch,\n",
    "              validation_data=(X_test, Y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(X_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n",
    "                        samples_per_epoch=X_train.shape[0],\n",
    "                        nb_epoch=nb_epoch,\n",
    "                        validation_data=(X_test, Y_test))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbpresent": {
   "slides": {
    "523a7dae-c51d-4dcf-8e26-1a88d05fa795": {
     "id": "523a7dae-c51d-4dcf-8e26-1a88d05fa795",
     "prev": "c7d07a25-8a15-4d9c-b1d4-a57fe844e2e5",
     "regions": {
      "b5818082-2dce-4a8b-a801-ccd7cb39b83b": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "3528348c-c1b8-4c09-9cb6-4dd5772c2a87",
        "part": "whole"
       },
       "id": "b5818082-2dce-4a8b-a801-ccd7cb39b83b"
      }
     }
    },
    "a0a6c0ee-bb21-4d9f-8886-42a5ebe48df0": {
     "id": "a0a6c0ee-bb21-4d9f-8886-42a5ebe48df0",
     "prev": "523a7dae-c51d-4dcf-8e26-1a88d05fa795",
     "regions": {
      "62d069a1-5662-4680-aa7f-60a2dd6b1a98": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f8c3e7fd-5711-429c-9398-a24c43950197",
        "part": "whole"
       },
       "id": "62d069a1-5662-4680-aa7f-60a2dd6b1a98"
      }
     }
    },
    "c7d07a25-8a15-4d9c-b1d4-a57fe844e2e5": {
     "id": "c7d07a25-8a15-4d9c-b1d4-a57fe844e2e5",
     "prev": "f09912f6-3947-462a-ac87-43abff450e5c",
     "regions": {
      "dd7feb31-2640-4e6c-9bfa-6066fbe4bf87": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "983e7694-97d7-4c99-abed-8cdfb0e78020",
        "part": "whole"
       },
       "id": "dd7feb31-2640-4e6c-9bfa-6066fbe4bf87"
      }
     }
    },
    "f09912f6-3947-462a-ac87-43abff450e5c": {
     "id": "f09912f6-3947-462a-ac87-43abff450e5c",
     "prev": null,
     "regions": {
      "262ac9d3-e3d3-47bc-8b19-ac08ab2e553e": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "4e5bbdc5-00cd-4457-a619-fa315f702739",
        "part": "whole"
       },
       "id": "262ac9d3-e3d3-47bc-8b19-ac08ab2e553e"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
